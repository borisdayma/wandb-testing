version: 0.0
name: huggingface-transformers
short: transformers
sources:
    - transformers:
        url: https://github.com/huggingface/transformers.git
        base: transformers
        hash: bfa4ccf77d65d8899b01417bd9845b2e78bc0ec5
        # v4.1.1 release
launch:
    path: transformers
    command:
        - python
        - examples/text-classification/run_glue.py
        - --model_name_or_path
        - bert-base-uncased
        - --task_name
        - MRPC
        - --do_train
        - --do_eval
        - --max_seq_length
        - 128
        - --per_gpu_train_batch_size
        - 32
        - --learning_rate
        - 2e-5
        - --max_steps
        - 1
        - --output_dir
        - /tmp/MRPC/
        - --overwrite_output_dir
        - --logging_steps
        - 1
    timeout: 15m
    killtime: 20m
environment:
    pip:
        - transformers==4.1.1
        - datasets
        - sklearn
    variables:
        WANDB_PROJECT: huggingface-demo
variants:
    - init:
        - python3s
        - wandb-cli
        - torches1plus
check:
    command:
        - python
        - check.py
